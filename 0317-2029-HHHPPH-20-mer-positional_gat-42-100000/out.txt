seq =  HHHPPHPHPHPPHPHPHPPH
seed =  42
algo =  positional_gat
save_path =  ./0317-2029-HHHPPH-20-mer-positional_gat-42-100000/
num_episodes =  100000
##### Summary of Hyperparameters #####
learning_rate:  0.0005
BATCH_SIZE:  32
GAMMA:  0.98
mem_start_train:  1000
TARGET_UPDATE:  100
buffer_limit:  10000
train_times:  10
##### End of Summary of Hyperparameters #####
decay_mode=exponential warmRestart=True
num_restarts=1 exploration_decay_rate=5 start_decay=0
initial state/obs:
((array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]), array([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0])), {'state': [(0, 0), (0, 1)]})
n_actions =  3
TransformerModel with:
inputs_size=4 hidden_size=128 num_layers=2 num_classes=3
found new highest reward =  1.0
{'chain_length': 20, 'seq_length': 20, 'actions': [0, 0, 2, 2, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 2, 0, 2, 1], 'state': [(0, 0), (0, 1), (-1, 1), (-1, 0), (-2, 0), (-2, 1), (-3, 1), (-3, 0), (-3, -1), (-4, -1), (-4, -2), (-5, -2), (-5, -3), (-5, -4), (-4, -4), (-4, -3), (-3, -3), (-3, -2), (-2, -2), (-1, -2)], 'first_turn_left': True, 'is_trapped': False}
Episode 0, score: 1.0, epsilon: 1.00, reward_max: 1.0
	s_prime: (tensor([[ 0.0000,  0.0000,  1.0000,  0.0000],
        [ 0.0000,  0.0500,  1.0000,  0.0000],
        [-0.0500,  0.0500,  1.0000,  0.0000]]), torch.Size([20, 4])), reward: 1.0, done: True, info: {'chain_length': 20, 'seq_length': 20, 'actions': [0, 0, 2, 2, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 2, 0, 2, 1], 'state': [(0, 0), (0, 1), (-1, 1), (-1, 0), (-2, 0), (-2, 1), (-3, 1), (-3, 0), (-3, -1), (-4, -1), (-4, -2), (-5, -2), (-5, -3), (-5, -4), (-4, -4), (-4, -3), (-3, -3), (-3, -2), (-2, -2), (-1, -2)], 'first_turn_left': True, 'is_trapped': False}
found new highest reward =  2.0
{'chain_length': 20, 'seq_length': 20, 'actions': [0, 0, 1, 1, 1, 1, 2, 0, 0, 2, 2, 1, 1, 2, 1, 0, 0, 1], 'state': [(0, 0), (0, 1), (-1, 1), (-1, 0), (-1, -1), (-1, -2), (-1, -3), (-1, -4), (-2, -4), (-2, -5), (-1, -5), (-1, -6), (-2, -6), (-3, -6), (-4, -6), (-4, -5), (-4, -4), (-5, -4), (-5, -5), (-5, -6)], 'first_turn_left': True, 'is_trapped': False}
found new highest reward =  3.0
{'chain_length': 20, 'seq_length': 20, 'actions': [0, 0, 1, 2, 2, 0, 1, 1, 0, 0, 1, 2, 0, 2, 2, 1, 2, 0], 'state': [(0, 0), (0, 1), (-1, 1), (-1, 0), (-1, -1), (-2, -1), (-2, 0), (-3, 0), (-4, 0), (-5, 0), (-5, -1), (-4, -1), (-3, -1), (-3, -2), (-2, -2), (-2, -3), (-3, -3), (-4, -3), (-4, -2), (-5, -2)], 'first_turn_left': True, 'is_trapped': False}
